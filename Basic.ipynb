{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cdfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-****\"\n",
    ")\n",
    "OPENAI_API_KEY = \"sk-****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ab7570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8TkwFjy2sRYpeUedbbphgIW1cdYcC', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Orange who?', role='assistant', function_call=None, tool_calls=None))], created=1702102635, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=3, prompt_tokens=35, total_tokens=38))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7c573",
   "metadata": {},
   "source": [
    "# ChatCompletion Object Description\n",
    "\n",
    "- **id**: `chatcmpl-8TkwFjy2sRYpeUedbbphgIW1cdYcC`\n",
    "  - 고유 식별자로, 특정 채팅 완료 사례를 식별하는 데 사용됩니다.\n",
    "  \n",
    "- **choices**: \n",
    "  - 각 `Choice`는 모델의 응답과 관련된 정보를 포함합니다.\n",
    "    - **finish_reason**: 'stop'\n",
    "      - 응답 생성이 종료된 이유를 나타냅니다.\n",
    "    - **index**: 0\n",
    "      - 해당 응답이 여러 응답 중 몇 번째인지 나타내는 인덱스입니다.\n",
    "    - **message**: 'Orange who?'\n",
    "      - 실제 모델이 생성한 메시지 내용입니다.\n",
    "    - **role**: 'assistant'\n",
    "      - 메시지를 생성한 역할을 나타냅니다.\n",
    "\n",
    "- **created**: 1702102635\n",
    "  - 채팅 완료가 생성된 시간을 유닉스 타임스탬프 형태로 나타냅니다.\n",
    "\n",
    "- **model**: 'gpt-3.5-turbo-0613'\n",
    "  - 사용된 모델의 버전을 나타냅니다.\n",
    "\n",
    "- **object**: 'chat.completion'\n",
    "  - 객체의 유형을 나타냅니다.\n",
    "\n",
    "- **system_fingerprint**: \n",
    "  - 시스템 지문에 대한 정보를 포함할 수 있습니다.\n",
    "\n",
    "- **usage**: \n",
    "  - 토큰 사용량에 대한 정보를 포함합니다.\n",
    "    - **completion_tokens**: 3\n",
    "    - **prompt_tokens**: 35\n",
    "    - **total_tokens**: 38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62df69cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange who?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8557f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me matey! Let me tell ye a tale of asynchronous programming, in the style of the fearsome pirate Blackbeard!\n",
      "\n",
      "Picture this, me hearties. In the vast ocean of programming, there be times when ye need to perform multiple tasks at once. But fear not, for asynchronous programming be here to save the day!\n",
      "\n",
      "Ye see, in traditional programming, ye be waitin' for one task to be done before movin' on to the next. But with asynchronous programming, ye can be takin' care of multiple tasks at the same time, like a true pirate multitasker!\n",
      "\n",
      "Instead of waitin' for a task to be completed, ye be sendin' it off on its own journey, while ye move on to the next task. It be like sendin' yer crewmates on different missions, each one doin' their own thing without waitin' for the others.\n",
      "\n",
      "But how does it work, ye ask? Well, in the world of programming, ye be usin' something called callbacks or promises. These be like secret messages ye be sendin' to yer crewmates, tellin' them what to do when they be finished with their tasks.\n",
      "\n",
      "Ye be givin' these messages to yer tasks, and they be goin' off on their own adventures. Meanwhile, ye be movin' on to the next task, without wastin' any time. When a task be done, it be sendin' ye a message, and ye be handlin' it accordingly.\n",
      "\n",
      "This be a powerful way to make yer code more efficient and responsive, me mateys. Ye can be performin' tasks in the background, while ye be attendin' to other important matters. It be like havin' a crew that be workin' together, but independently.\n",
      "\n",
      "So, me hearties, if ye be lookin' to conquer the vast seas of programming, ye best be learnin' the ways of asynchronous programming. It be a valuable skill that be makin' ye a true pirate of the code!\n",
      "\n",
      "Now, set sail on yer programming adventures, and may the winds of asynchronous programming be at yer back, me mateys! Arrr!\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f272c",
   "metadata": {},
   "source": [
    "# 시스템 메시지\n",
    "\n",
    "\n",
    "시스템 메시지는 비서에게 다양한 성격이나 행동을 준비하는 데 사용될 수 있습니다.\n",
    "\n",
    "\n",
    "는 일반적으로 또는 gpt-3.5-turbo-0301처럼 시스템 메시지에 많은 주의를 기울이지 않습니다 . 따라서 의 경우 대신 사용자 메시지에 중요한 지침을 배치하는 것이 좋습니다. 일부 개발자는 대화가 길어질수록 모델의 주의가 다른 곳으로 흩어지는 것을 방지하기 위해 대화가 끝날 무렵 시스템 메시지를 지속적으로 이동하는 데 성공했습니다.gpt-4-0314gpt-3.5-turbo-0613gpt-3.5-turbo-0301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7108fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Fractions are a way to represent parts of a whole. They are made up of two numbers: a numerator and a denominator. The numerator tells you how many parts you have, and the denominator tells you how many equal parts make up the whole.\n",
      "\n",
      "Let's take an example to understand this better. Imagine you have a pizza that is divided into 8 equal slices. If you eat 3 slices, you can represent that as the fraction 3/8. Here, the numerator is 3 because you ate 3 slices, and the denominator is 8 because the whole pizza is divided into 8 slices.\n",
      "\n",
      "Fractions can also be used to represent numbers less than 1. For example, if you eat half of a pizza, you can write it as 1/2. Here, the numerator is 1 because you ate one slice, and the denominator is 2 because the whole pizza is divided into 2 equal parts.\n",
      "\n",
      "Now, let's talk about equivalent fractions. Equivalent fractions are different fractions that represent the same amount. For example, 1/2 and 2/4 are equivalent fractions because they both represent half of something. To find equivalent fractions, you can multiply or divide both the numerator and denominator by the same number.\n",
      "\n",
      "Here's a question to check your understanding: If you have a cake divided into 12 equal slices and you eat 4 slices, what fraction of the cake did you eat?\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to explain concepts in great depth\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958776dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions represent a part of a whole or a ratio between two numbers. They consist of a numerator (top number) and a denominator (bottom number), separated by a slash (/). The numerator represents the number of parts we have, while the denominator represents the total number of equal parts in the whole.\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
    "        {\"role\": \"system\", \"content\": \"당신은 간결한 조수입니다. 짧고 핵심적인 대답으로만 답변하며, 추가 설명은 하지 않습니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32406e64",
   "metadata": {},
   "source": [
    "# 퓨샷 프롬프트\n",
    "\n",
    "어떤 경우에는 모델에게 원하는 것을 말하는 것보다 원하는 것을 모델에 보여주는 것이 더 쉽습니다.\n",
    "\n",
    "\n",
    "원하는 것을 모델에 표시하는 한 가지 방법은 가짜 예제 메시지를 사용하는 것입니다.\n",
    "\n",
    "\n",
    "예를 들어:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ab2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26b3d4",
   "metadata": {},
   "source": [
    "# 4. 토큰 계산\n",
    "\n",
    "요청을 제출하면 API는 메시지를 일련의 토큰으로 변환합니다.\n",
    "\n",
    "\n",
    "사용되는 토큰 수는 다음에 영향을 미칩니다.\n",
    "\n",
    "\n",
    "요청 비용\n",
    "응답을 생성하는 데 걸리는 시간\n",
    "gpt-3.5-turbo최대 토큰 제한(4,096개 또는 8,192개 gpt-4) 에 도달하여 응답이 잘리는 경우\n",
    "다음 함수를 사용하여 메시지 목록에서 사용할 토큰 수를 계산할 수 있습니다.\n",
    "\n",
    "\n",
    "메시지에서 토큰을 계산하는 정확한 방법은 모델마다 다를 수 있습니다. 시간을 초월한 보장이 아닌 추정치 아래 함수의 개수를 고려하세요.\n",
    "\n",
    "\n",
    "특히 선택적 함수 입력을 사용하는 요청은 아래 계산된 추정치 외에 추가 토큰을 소비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e38d2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0afb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "127 prompt tokens counted by num_tokens_from_messages().\n",
      "This\n",
      "gpt-3.5-turbo-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "This\n",
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "This\n",
      "gpt-4-0314\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "Changing\n",
      "gpt-4-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "This\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    \n",
    "    print(f'{response.choices[0].message.content}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c6e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
